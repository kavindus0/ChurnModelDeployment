# ğŸ“š **Sample Enhanced Notebook Section**

## ğŸ”§ **0_data_prep.ipynb - Complete Enhanced Version**

```markdown
# ğŸ”§ **Step 0: Data Preparation & Preprocessing**
## *Building the Foundation for Churn Prediction*

---

### ğŸ“‹ **What We'll Accomplish Today:**

| Step | Task | Goal | Time |
|------|------|------|------|
| 1ï¸âƒ£ | **Load Data** | Get our customer dataset | 2 min |
| 2ï¸âƒ£ | **Explore Data** | Understand what we have | 5 min |
| 3ï¸âƒ£ | **Clean Data** | Fix missing values & errors | 10 min |
| 4ï¸âƒ£ | **Transform Features** | Prepare data for ML | 15 min |
| 5ï¸âƒ£ | **Split Data** | Create train/test sets | 3 min |
| 6ï¸âƒ£ | **Balance Data** | Handle class imbalance | 5 min |

**Total Time:** ~40 minutes

---

### ğŸ§  **Memory Aid - "PREP" Method:**
- **P**ick up the data (Load)
- **R**eview what you have (Explore) 
- **E**liminate problems (Clean)
- **P**repare for modeling (Transform)

---

### ğŸ¯ **Learning Objectives:**
By the end of this notebook, you will:
- âœ… Load and explore customer churn data
- âœ… Handle missing values and outliers
- âœ… Transform categorical and numerical features
- âœ… Create balanced train/test datasets
- âœ… Save preprocessed data for modeling
```

```python
# ===== SECTION 1: LIBRARY IMPORTS =====

# ğŸ§  Memory Aid: "DATA-VIZ-ML-PREP-BALANCE"
# DATA: numpy, pandas
# VIZ: matplotlib  
# ML: sklearn pipeline
# PREP: preprocessing tools
# BALANCE: SMOTE for class balance

# ===== CORE DATA LIBRARIES =====
import numpy as np                    # ğŸ”¢ Numbers & arrays
import pandas as pd                   # ğŸ“Š Data tables & analysis
import warnings                       # âš ï¸ Control warning messages
warnings.filterwarnings('ignore')

# ===== VISUALIZATION =====
import matplotlib.pyplot as plt       # ğŸ“ˆ Charts & graphs
import seaborn as sns                 # ğŸ¨ Beautiful statistical plots
plt.style.use('seaborn-v0_8')        # ğŸ¨ Set attractive plot style

# ===== MACHINE LEARNING PIPELINE =====
from sklearn.pipeline import Pipeline              # ğŸ”§ ML workflow organizer
from sklearn.compose import ColumnTransformer      # ğŸ¯ Feature transformer
from sklearn.model_selection import train_test_split # âœ‚ï¸ Data splitter

# ===== DATA PREPROCESSING =====
from sklearn.impute import SimpleImputer           # ğŸ”§ Fill missing values
from sklearn.preprocessing import (                 # ğŸ¨ Data transformers
    OneHotEncoder,      # For categories (Gender, Geography)
    OrdinalEncoder,     # For ordered categories (CreditScoreBins)  
    StandardScaler      # For numbers (Age, Balance, etc.)
)

# ===== CLASS BALANCING =====
from imblearn.over_sampling import SMOTE           # âš–ï¸ Balance churned vs non-churned

# ===== SUCCESS MESSAGE =====
print("ğŸ‰ SUCCESS: All libraries loaded!")
print("ğŸ“š Libraries imported:")
print("   ğŸ“Š Data: numpy, pandas")  
print("   ğŸ“ˆ Visualization: matplotlib, seaborn")
print("   ğŸ¤– ML: sklearn pipeline & preprocessing")
print("   âš–ï¸ Balancing: SMOTE")
print("ğŸš€ Ready to start data preparation!")
```

```markdown
## 1ï¸âƒ£ **Load Raw Data**

### ğŸ“‚ **Getting Our Customer Dataset**

**What we're doing:** Loading bank customer data to predict who might leave (churn)

**Memory Aid - "LOAD":**
- **L**ocate the file
- **O**pen with pandas  
- **A**ssess the shape
- **D**isplay first few rows

### ğŸ“‹ **Expected Dataset Structure:**
| Column | Type | Description |
|--------|------|-------------|
| CustomerId | int | Unique customer identifier |
| Surname | object | Customer last name |
| CreditScore | int | Credit rating (300-850) |
| Geography | object | Country (France, Germany, Spain) |
| Gender | object | Male or Female |
| Age | int | Customer age |
| Tenure | int | Years with bank |
| Balance | float | Account balance |
| NumOfProducts | int | Number of bank products |
| HasCrCard | int | Has credit card (0/1) |
| IsActiveMember | int | Active member (0/1) |
| EstimatedSalary | float | Estimated annual salary |
| Exited | int | **TARGET**: Left bank (0/1) |
```

```python
# ===== SECTION 2: DATA LOADING =====

print("ğŸ“‚ LOADING CUSTOMER CHURN DATASET...")
print("=" * 60)

# ğŸ“‚ Load the customer churn dataset
try:
    df = pd.read_csv('data/dataset.csv')
    print("âœ… SUCCESS: Dataset loaded successfully!")
except FileNotFoundError:
    print("âŒ ERROR: Dataset file not found!")
    print("ğŸ’¡ TIP: Make sure 'data/dataset.csv' exists")
    raise

# ğŸ“Š Dataset Overview
print(f"ğŸ“ Dataset Shape: {df.shape[0]:,} rows Ã— {df.shape[1]} columns")
print(f"ğŸ’¾ Memory Usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB")

# ğŸ¯ Target Variable Check
if 'Exited' in df.columns:
    churn_rate = df['Exited'].mean() * 100
    print(f"ğŸ¯ Churn Rate: {churn_rate:.1f}% ({df['Exited'].sum():,} out of {len(df):,} customers)")
    
    # Class distribution
    class_counts = df['Exited'].value_counts()
    print(f"   ğŸ“Š Stayed: {class_counts[0]:,} customers ({(class_counts[0]/len(df)*100):.1f}%)")
    print(f"   ğŸ“Š Churned: {class_counts[1]:,} customers ({(class_counts[1]/len(df)*100):.1f}%)")
else:
    print("âš ï¸ WARNING: 'Exited' column not found!")

print("=" * 60)
```

```python
# ğŸ‘€ First look at our data
print("ğŸ” FIRST 5 CUSTOMERS:")
display(df.head())

print("\nğŸ“‹ DATASET INFORMATION:")
print("-" * 60)
print(f"{'Column':<20} {'Type':<15} {'Non-Null':<10} {'Unique':<8} {'Sample'}")
print("-" * 60)

for col in df.columns:
    non_null = df[col].count()
    unique_count = df[col].nunique()
    sample_val = str(df[col].iloc[0])
    if len(sample_val) > 15:
        sample_val = sample_val[:12] + "..."
    
    print(f"{col:<20} {str(df[col].dtype):<15} {non_null:<10} {unique_count:<8} {sample_val}")

print("-" * 60)
```

```markdown
## 2ï¸âƒ£ **Explore Data Quality**

### ğŸ” **Data Quality Assessment**

**Memory Aid - "CHECK":**
- **C**ount missing values
- **H**unt for duplicates  
- **E**xamine data types
- **C**atch outliers
- **K**eep notes on issues

### ğŸ“Š **Quality Metrics We'll Check:**
| Metric | What It Tells Us | Action Needed |
|--------|------------------|---------------|
| Missing Values | Data completeness | Impute or remove |
| Duplicates | Data integrity | Remove duplicates |
| Outliers | Data validity | Cap or transform |
| Data Types | Correct format | Convert if needed |
```

```python
# ===== SECTION 3: DATA QUALITY EXPLORATION =====

print("ğŸ” DATA QUALITY ASSESSMENT")
print("=" * 60)

# ğŸ“Š Missing Values Analysis
print("1ï¸âƒ£ MISSING VALUES CHECK:")
missing_data = df.isnull().sum()
missing_percent = (missing_data / len(df)) * 100

missing_df = pd.DataFrame({
    'Column': missing_data.index,
    'Missing_Count': missing_data.values,
    'Missing_Percent': missing_percent.values
}).sort_values('Missing_Count', ascending=False)

if missing_data.sum() == 0:
    print("âœ… EXCELLENT: No missing values found!")
else:
    print("âš ï¸ Missing values detected:")
    for _, row in missing_df[missing_df['Missing_Count'] > 0].iterrows():
        print(f"   {row['Column']}: {row['Missing_Count']} ({row['Missing_Percent']:.1f}%)")

print("\n2ï¸âƒ£ DUPLICATE RECORDS CHECK:")
duplicates = df.duplicated().sum()
if duplicates == 0:
    print("âœ… EXCELLENT: No duplicate records found!")
else:
    print(f"âš ï¸ Found {duplicates} duplicate records")

print("\n3ï¸âƒ£ DATA TYPES CHECK:")
print("ğŸ“‹ Current data types:")
for col, dtype in df.dtypes.items():
    print(f"   {col:<20}: {dtype}")

print("\n4ï¸âƒ£ BASIC STATISTICS:")
display(df.describe())

print("=" * 60)
```

This is just the beginning of the enhanced notebook. Each section would continue with:

1. **Visual data exploration** with enhanced plots
2. **Feature engineering** with clear explanations
3. **Data preprocessing** with progress indicators
4. **Train/test splitting** with validation
5. **Class balancing** with before/after comparisons
6. **Summary section** with key takeaways

Would you like me to continue with the complete enhanced versions of all your notebooks?