{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Step 0: Data Preparation and Preprocessing (Fixed)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Import Dependencies\n",
                "\n",
                "First, we need to load all the tools (libraries) we need for our project."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "from matplotlib import pyplot as plt\n",
                "from sklearn.pipeline import Pipeline\n",
                "from imblearn.over_sampling import SMOTE\n",
                "from sklearn.compose import ColumnTransformer\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.impute import SimpleImputer\n",
                "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Load Raw Data\n",
                "\n",
                "Here, we load our customer dataset from the file."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df = pd.read_csv('data/raw/dataset.csv')\n",
                "print(\"Successfully loaded dataset.\")\n",
                "print(f\"Shape: {df.shape}\")\n",
                "print(\"First 5 rows:\")\n",
                "display(df.head())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Feature Engineering and Preprocessing\n",
                "\n",
                "This is where we clean and prepare our data for the model.\n",
                "\n",
                "### Mnemonic: **P-P-T** (Pipelines, Preprocessor, Transform)\n",
                "*   **P** - **Pipelines**: Create separate cleaning steps for different types of data (numbers, categories).\n",
                "*   **P** - **Preprocessor**: Combine all the pipelines into one single tool.\n",
                "*   **T** - **Transform**: Use the preprocessor to clean the entire dataset."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3.1. Define Feature Categories\n",
                "\n",
                "We group our columns based on their data type."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define feature categories according to product requirements\n",
                "numerical_features = ['Age', 'Tenure', 'Balance', 'EstimatedSalary']\n",
                "nominal_features = ['Gender', 'Geography']\n",
                "ordinal_features = ['CreditScoreBins']\n",
                "remainder_features = ['NumOfProducts', 'HasCrCard', 'IsActiveMember']\n",
                "target_variable = 'Exited'\n",
                "\n",
                "print(f\"Numerical features: {numerical_features}\")\n",
                "print(f\"Nominal features: {nominal_features}\")\n",
                "print(f\"Ordinal features: {ordinal_features}\")\n",
                "print(f\"Remainder features: {remainder_features}\")\n",
                "print(f\"Target variable: {target_variable}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3.2. Create Preprocessing Pipelines\n",
                "\n",
                "We create a set of steps (a \"pipeline\") for each data type to handle missing values and scale the data correctly."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create preprocessing pipelines for different feature types\n",
                "numerical_transformer = Pipeline(\n",
                "    steps=[\n",
                "        ('imputer', SimpleImputer(strategy='median')),\n",
                "        ('scaler', StandardScaler())\n",
                "    ]\n",
                ")\n",
                "\n",
                "nominal_transformer = Pipeline(\n",
                "    steps=[\n",
                "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
                "        ('encoder', OneHotEncoder(drop='first', handle_unknown='ignore'))\n",
                "    ]\n",
                ")\n",
                "\n",
                "ordinal_transformer = Pipeline(\n",
                "    steps=[\n",
                "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
                "        ('encoder', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))\n",
                "    ]\n",
                ")\n",
                "\n",
                "print(\"‚úÖ Preprocessing pipelines created successfully\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3.3. Combine Pipelines with ColumnTransformer\n",
                "\n",
                "Now, we combine all our pipelines into a single, powerful preprocessor."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create the main preprocessor using ColumnTransformer\n",
                "preprocessor = ColumnTransformer(\n",
                "    transformers=[\n",
                "        ('num', numerical_transformer, numerical_features),\n",
                "        ('nom', nominal_transformer, nominal_features),\n",
                "        ('ord', ordinal_transformer, ordinal_features),\n",
                "    ],\n",
                "    remainder='passthrough'  # Keep the remainder features as-is\n",
                ")\n",
                "\n",
                "print(\"‚úÖ ColumnTransformer created successfully\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3.4. Apply Transformations and Create Final DataFrame\n",
                "\n",
                "We use our preprocessor to transform the data and create a final, clean DataFrame."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# üõ°Ô∏è Step 1: Create safe copy of original data\n",
                "print(\"üìã Creating safe copy of data...\")\n",
                "df_copy = df.copy()\n",
                "\n",
                "# üîß Step 2: Apply all preprocessing transformations\n",
                "print(\"‚öôÔ∏è Applying preprocessing pipeline...\")\n",
                "transformed_array = preprocessor.fit_transform(df_copy)\n",
                "print(f\"‚úÖ Transformed shape: {transformed_array.shape}\")\n",
                "\n",
                "# üè∑Ô∏è Step 3: Reconstruct column names (sklearn loses them)\n",
                "print(\"üè∑Ô∏è Reconstructing column names...\")\n",
                "\n",
                "# Get the feature names from each transformer\n",
                "try:\n",
                "    # Get expanded names for one-hot encoded features\n",
                "    nominal_expanded_names = preprocessor.named_transformers_['nom']['encoder'].get_feature_names_out(nominal_features)\n",
                "    \n",
                "    # The ColumnTransformer outputs columns in the order they were defined:\n",
                "    # 1. numerical features (4 columns)\n",
                "    # 2. nominal features (one-hot encoded, variable number)\n",
                "    # 3. ordinal features (1 column) \n",
                "    # 4. remainder features (3 columns) - these are added by remainder='passthrough'\n",
                "    \n",
                "    final_feature_names = (\n",
                "        numerical_features +           # Age, Tenure, Balance, EstimatedSalary\n",
                "        list(nominal_expanded_names) + # Gender_Male, Geography_France, etc.\n",
                "        ordinal_features +             # CreditScoreBins  \n",
                "        remainder_features             # NumOfProducts, HasCrCard, IsActiveMember\n",
                "    )\n",
                "    \n",
                "    print(f\"üìä Transformed array shape: {transformed_array.shape}\")\n",
                "    print(f\"üìä Total feature names: {len(final_feature_names)}\")\n",
                "    print(f\"   Numerical: {len(numerical_features)} features\")\n",
                "    print(f\"   Nominal (expanded): {len(nominal_expanded_names)} features\")\n",
                "    print(f\"   Ordinal: {len(ordinal_features)} features\")\n",
                "    print(f\"   Remainder: {len(remainder_features)} features\")\n",
                "    \n",
                "    # Check if shapes match\n",
                "    if transformed_array.shape[1] != len(final_feature_names):\n",
                "        print(f\"‚ö†Ô∏è  Shape mismatch detected!\")\n",
                "        print(f\"   Array columns: {transformed_array.shape[1]}\")\n",
                "        print(f\"   Feature names: {len(final_feature_names)}\")\n",
                "        \n",
                "        # Use generic column names as fallback\n",
                "        final_feature_names = [f'feature_{i}' for i in range(transformed_array.shape[1])]\n",
                "        print(f\"   Using generic names: {len(final_feature_names)} columns\")\n",
                "        \n",
                "except Exception as e:\n",
                "    print(f\"‚ö†Ô∏è  Error in column naming: {e}\")\n",
                "    # Fallback to generic column names\n",
                "    final_feature_names = [f'feature_{i}' for i in range(transformed_array.shape[1])]\n",
                "    print(f\"   Using generic names: {len(final_feature_names)} columns\")\n",
                "\n",
                "# üìä Step 4: Convert back to DataFrame with proper column names\n",
                "df_features = pd.DataFrame(transformed_array, columns=final_feature_names)\n",
                "\n",
                "# üéØ Step 5: Add target variable back\n",
                "df_final = pd.concat([\n",
                "    df_features, \n",
                "    df[target_variable].reset_index(drop=True)\n",
                "], axis=1)\n",
                "\n",
                "print(\"üéâ Preprocessing complete!\")\n",
                "print(f\"üìè Final dataset shape: {df_final.shape}\")\n",
                "display(df_final.head())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Handle Class Imbalance using SMOTE\n",
                "\n",
                "Our data has many more 'Not Churn' customers than 'Churn' customers. This can bias our model. We use **SMOTE** to create new, synthetic 'Churn' data points to balance things out.\n",
                "\n",
                "### Mnemonic: **S-B-V** (Split, Balance, Visualize)\n",
                "*   **S** - **Split**: First, split data into training and testing sets.\n",
                "*   **B** - **Balance**: Apply SMOTE *only* to the training data.\n",
                "*   **V** - **Visualize**: Create charts to see the data before and after balancing."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Separate features and target\n",
                "X = df_final.drop(columns=[target_variable])\n",
                "Y = df_final[target_variable]\n",
                "\n",
                "print(f\"Features shape: {X.shape}\")\n",
                "print(f\"Target shape: {Y.shape}\")\n",
                "print(f\"Target distribution:\")\n",
                "print(Y.value_counts())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Split the data into training and testing sets\n",
                "X_train, X_test, Y_train, Y_test = train_test_split(\n",
                "    X, Y, \n",
                "    test_size=0.2, \n",
                "    random_state=42, \n",
                "    stratify=Y\n",
                ")\n",
                "\n",
                "print(f\"Training set shape: {X_train.shape}\")\n",
                "print(f\"Test set shape: {X_test.shape}\")\n",
                "print(\"\\nClass distribution before SMOTE:\")\n",
                "print(Y_train.value_counts())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Apply SMOTE to balance the training data\n",
                "smote = SMOTE(random_state=42)\n",
                "X_train_resampled, Y_train_resampled = smote.fit_resample(X_train, Y_train)\n",
                "\n",
                "print(\"\\nClass distribution after SMOTE:\")\n",
                "print(pd.Series(Y_train_resampled).value_counts())\n",
                "print(f\"\\nResampled training set shape: {X_train_resampled.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4.1. Visualize Class Distribution"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create visualization of class distribution before and after SMOTE\n",
                "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
                "fig.suptitle('Class Distribution Comparison', fontsize=16)\n",
                "\n",
                "# Before SMOTE\n",
                "Y_train.value_counts().plot(kind='bar', ax=axes[0], color=['skyblue', 'salmon'])\n",
                "axes[0].set_title('Before SMOTE (Training Set)')\n",
                "axes[0].set_ylabel('Count')\n",
                "axes[0].set_xlabel('Class (0=Not Churned, 1=Churned)')\n",
                "axes[0].tick_params(axis='x', rotation=0)\n",
                "\n",
                "# After SMOTE\n",
                "pd.Series(Y_train_resampled).value_counts().plot(kind='bar', ax=axes[1], color=['skyblue', 'salmon'])\n",
                "axes[1].set_title('After SMOTE (Training Set)')\n",
                "axes[1].set_ylabel('Count')\n",
                "axes[1].set_xlabel('Class (0=Not Churned, 1=Churned)')\n",
                "axes[1].tick_params(axis='x', rotation=0)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Save Processed Data\n",
                "\n",
                "Let's save our processed data for use in subsequent notebooks."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create artifacts directory if it doesn't exist\n",
                "import os\n",
                "os.makedirs('artifacts', exist_ok=True)\n",
                "\n",
                "# Save the processed datasets\n",
                "np.savez('artifacts/X_train.npz', X_train_resampled)\n",
                "np.savez('artifacts/Y_train.npz', Y_train_resampled)\n",
                "np.savez('artifacts/X_test.npz', X_test)\n",
                "np.savez('artifacts/Y_test.npz', Y_test)\n",
                "\n",
                "print(\"‚úÖ Processed data saved to artifacts/\")\n",
                "print(f\"   X_train_resampled: {X_train_resampled.shape}\")\n",
                "print(f\"   Y_train_resampled: {Y_train_resampled.shape}\")\n",
                "print(f\"   X_test: {X_test.shape}\")\n",
                "print(f\"   Y_test: {Y_test.shape}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}